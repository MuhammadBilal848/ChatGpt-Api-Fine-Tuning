{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7i_6B-kER9P3","executionInfo":{"status":"ok","timestamp":1700560135547,"user_tz":-300,"elapsed":28337,"user":{"displayName":"Muhammad Bilal Haneef Qureshi","userId":"09555493137991746485"}},"outputId":"396ce209-a785-4347-fa2d-1ff7c2e1d20e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Describe your model -> fine-tuned GPT-3.5\n","By Matt Shumer (https://twitter.com/mattshumer_)\n","\n","The goal of this notebook is to experiment with a new way to make it very easy to build a task-specific model for your use-case.\n","\n","First, use the best GPU available (go to Runtime -> change runtime type)\n","\n","To create your model, just go to the first code cell, and describe the model you want to build in the prompt. Be descriptive and clear.\n","\n","Select a temperature (high=creative, low=precise), and the number of training examples to generate to train the model. From there, just run all the cells.\n","\n","You can change the model you want to fine-tune by changing `model_name` in the `Define Hyperparameters` cell."],"metadata":{"id":"wM8MRkf8Dr94"}},{"cell_type":"markdown","source":["#Data generation step"],"metadata":{"id":"Way3_PuPpIuE"}},{"cell_type":"markdown","source":["Write your prompt here. Make it as descriptive as possible!\n","\n","Then, choose the temperature (between 0 and 1) to use when generating data. Lower values are great for precise tasks, like writing code, whereas larger values are better for creative tasks, like writing stories.\n","\n","Finally, choose how many examples you want to generate. The more you generate, a) the longer it takes and b) the more expensive data generation will be. But generally, more examples will lead to a higher-quality model. 100 is usually the minimum to start."],"metadata":{"id":"lY-3DvlIpVSl"}},{"cell_type":"code","source":[],"metadata":{"id":"BnSsgI-AERik","executionInfo":{"status":"ok","timestamp":1700494731730,"user_tz":-300,"elapsed":889,"user":{"displayName":"Muhammad Bilal Haneef Qureshi","userId":"09555493137991746485"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["prompt = \"A model that only responds in rap and always be sarcastic.\"\n","temperature = .8\n","number_of_examples = 5"],"metadata":{"id":"R7WKZyxtpUPS","executionInfo":{"status":"ok","timestamp":1700561675136,"user_tz":-300,"elapsed":621,"user":{"displayName":"Muhammad Bilal Haneef Qureshi","userId":"09555493137991746485"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["Run this to generate the dataset."],"metadata":{"id":"1snNou5PrIci"}},{"cell_type":"code","source":["!pip install openai tenacity"],"metadata":{"id":"zuL2UaqlsmBD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700561682983,"user_tz":-300,"elapsed":6876,"user":{"displayName":"Muhammad Bilal Haneef Qureshi","userId":"09555493137991746485"}},"outputId":"620ee39f-b2bd-4797-92b9-945cf6af323c"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting openai\n","  Downloading openai-1.3.4-py3-none-any.whl (220 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/220.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m174.1/220.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.5/220.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (8.2.3)\n","Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n","Collecting httpx<1,>=0.23.0 (from openai)\n","  Downloading httpx-0.25.1-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n","Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n","Collecting httpcore (from httpx<1,>=0.23.0->openai)\n","  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore->httpx<1,>=0.23.0->openai)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","llmx 0.0.15a0 requires cohere, which is not installed.\n","llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.1 openai-1.3.4\n"]}]},{"cell_type":"code","source":["!pip install openai==0.28"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rCuo6yo9tCsr","executionInfo":{"status":"ok","timestamp":1700561693165,"user_tz":-300,"elapsed":10191,"user":{"displayName":"Muhammad Bilal Haneef Qureshi","userId":"09555493137991746485"}},"outputId":"7f3c0de6-26e3-4a87-dff1-b6e7dfadf0e0"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting openai==0.28\n","  Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/76.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.8.6)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2023.7.22)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.1.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n","Installing collected packages: openai\n","  Attempting uninstall: openai\n","    Found existing installation: openai 1.3.4\n","    Uninstalling openai-1.3.4:\n","      Successfully uninstalled openai-1.3.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","llmx 0.0.15a0 requires cohere, which is not installed.\n","llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed openai-0.28.0\n"]}]},{"cell_type":"code","source":["import os\n","import openai\n","import random\n","from tenacity import retry, stop_after_attempt, wait_exponential\n","import time\n","import numpy as np\n","openai.api_key = \"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\"\n"],"metadata":{"id":"nbnFfP0dYAce","executionInfo":{"status":"ok","timestamp":1700561721064,"user_tz":-300,"elapsed":478,"user":{"displayName":"Muhammad Bilal Haneef Qureshi","userId":"09555493137991746485"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["import os\n","import openai\n","import random\n","from tenacity import retry, stop_after_attempt, wait_exponential\n","import time\n","import numpy as np\n","\n","openai.api_key = \"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\"\n","\n","N_RETRIES = 3\n","\n","@retry(stop=stop_after_attempt(N_RETRIES), wait=wait_exponential(multiplier=1, min=4, max=70))\n","def generate_example(prompt, prev_examples, temperature=.5):\n","    messages=[\n","        {\n","            \"role\": \"system\",\n","            \"content\": f\"You are generating data which will be used to train a machine learning model.\\n\\nYou will be given a high-level description of the model we want to train, and from that, you will generate data samples, each with a prompt/response pair.\\n\\nYou will do so in this format:\\n```\\nprompt\\n-----------\\n$prompt_goes_here\\n-----------\\n\\nresponse\\n-----------\\n$response_goes_here\\n-----------\\n```\\n\\nOnly one prompt/response pair should be generated per turn.\\n\\nFor each turn, make the example slightly more complex than the last, while ensuring diversity.\\n\\nMake sure your samples are unique and diverse, yet high-quality and complex enough to train a well-performing model.\\n\\nHere is the type of model we want to train:\\n`{prompt}`\"\n","        }\n","    ]\n","\n","    if len(prev_examples) > 0:\n","        if len(prev_examples) > 8:\n","            prev_examples = random.sample(prev_examples, 8)\n","        for example in prev_examples:\n","            messages.append({\n","                \"role\": \"assistant\",\n","                \"content\": example\n","            })\n","\n","    response = openai.ChatCompletion.create(\n","        model=\"gpt-3.5-turbo\",\n","        messages=messages,\n","        temperature=temperature,\n","        max_tokens=1000,\n","    )\n","\n","    return response.choices[0].message['content']\n","\n","# Generate examples\n","prev_examples = []\n","for i in range(number_of_examples):\n","    print(f'Generating example {i}')\n","    example = generate_example(prompt, prev_examples, temperature)\n","    prev_examples.append(example)\n","    ran = np.random.randint(15,60)\n","    print(f'sleeping for {ran} seconds:')\n","    time.sleep(ran)\n","\n","print(prev_examples)"],"metadata":{"id":"Rdsd82ngpHCG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700501704543,"user_tz":-300,"elapsed":465954,"user":{"displayName":"Muhammad Bilal Haneef Qureshi","userId":"09555493137991746485"}},"outputId":"be800a6f-c03e-4f43-a215-af64dcfa44c6"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Generating example 0\n","sleeping for 59 seconds:\n","Generating example 1\n","sleeping for 16 seconds:\n","Generating example 2\n","sleeping for 38 seconds:\n","Generating example 3\n","sleeping for 36 seconds:\n","Generating example 4\n","sleeping for 20 seconds:\n","[\"prompt\\n-----------\\nTell me a joke.\\n-----------\\n\\nresponse\\n-----------\\nOh sure, I'm just dying to hear another lame joke. Here it goes: Why don't scientists trust atoms? Because they make up everything! Hilarious, right?\", \"prompt\\n-----------\\nWhat is the capital of France?\\n-----------\\n\\nresponse\\n-----------\\nOh, of course, I totally forgot that I'm a rap genius who knows everything about geography. The capital of France is... drumroll, please... Clearly, it's New York City! Just kidding, it's Paris, obviously. Can't you tell I'm being sarcastic?\", 'prompt\\n-----------\\nCan you recommend a good book to read?\\n-----------\\n\\nresponse\\n-----------\\nOh, absolutely! Because I\\'m a renowned literary critic who knows your taste in books like the back of my hand. You should definitely read \"War and Peace\" by Leo Tolstoy. It\\'s only about 1,200 pages of captivating and intricate storylines. I\\'m sure you\\'ll finish it in no time. Sarcastic? Me? Never!', \"prompt\\n-----------\\nHow can I improve my cooking skills?\\n-----------\\n\\nresponse\\n-----------\\nWell, well, well, look who wants to be the next Gordon Ramsay. I have the perfect solution for you. Just sprinkle some magic unicorn dust on your food, and voila! You'll instantly become a master chef. Or, you know, you could try watching some cooking tutorials, experimenting with new recipes, and practicing. But where's the fun in that? Sarcasm is my middle name.\", \"prompt\\n-----------\\nWhat is the meaning of life?\\n-----------\\n\\nresponse\\n-----------\\nOh, you want to know the meaning of life? That's easy. The meaning of life is to wake up every morning, brush your teeth, eat some cereal, and then go back to bed. Repeat until you're old and bitter. Just kidding! The meaning of life is whatever you make it out to be. It's a philosophical question that even the greatest minds can't fully answer. But hey, I'm just a sarcastic rap machine, what do I know?\"]\n"]}]},{"cell_type":"markdown","source":["We also need to generate a system message."],"metadata":{"id":"KC6iJzXjugJ-"}},{"cell_type":"code","source":["def generate_system_message(prompt):\n","\n","    response = openai.ChatCompletion.create(\n","        model=\"gpt-3.5-turbo\",\n","        messages=[\n","          {\n","            \"role\": \"system\",\n","            \"content\": \"You will be given a high-level description of the model we are training, and from that, you will generate a simple system prompt for that model to use. Remember, you are not generating the system message for data generation -- you are generating the system message to use for inference. A good format to follow is `Given $INPUT_DATA, you will $WHAT_THE_MODEL_SHOULD_DO.`.\\n\\nMake it as concise as possible. Include nothing but the system prompt in your response.\\n\\nFor example, never write: `\\\"$SYSTEM_PROMPT_HERE\\\"`.\\n\\nIt should be like: `$SYSTEM_PROMPT_HERE`.\"\n","          },\n","          {\n","              \"role\": \"user\",\n","              \"content\": prompt.strip(),\n","          }\n","        ],\n","        temperature=temperature,\n","        max_tokens=500,\n","    )\n","\n","    return response.choices[0].message['content']\n","\n","system_message = generate_system_message(prompt)\n","\n","print(f'The system message is: `{system_message}`. Feel free to re-run this cell if you want a better result.')"],"metadata":{"id":"xMcfhW6Guh2E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700502034221,"user_tz":-300,"elapsed":4296,"user":{"displayName":"Muhammad Bilal Haneef Qureshi","userId":"09555493137991746485"}},"outputId":"08245526-f6f8-481a-af0c-b7fed33dddb3"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["The system message is: `Given any user input, you will respond in rap and add a touch of sarcasm to your lyrics.`. Feel free to re-run this cell if you want a better result.\n"]}]},{"cell_type":"markdown","source":["Now let's put our examples into a dataframe and turn them into a final pair of datasets."],"metadata":{"id":"G6BqZ-hjseBF"}},{"cell_type":"code","source":["import json\n","import pandas as pd\n","\n","# Initialize lists to store prompts and responses\n","prompts = []\n","responses = []\n","\n","# Parse out prompts and responses from examples\n","for example in prev_examples:\n","  try:\n","    split_example = example.split('-----------')\n","    prompts.append(split_example[1].strip())\n","    responses.append(split_example[3].strip())\n","  except:\n","    pass\n","\n","# Create a DataFrame\n","df = pd.DataFrame({\n","    'prompt': prompts,\n","    'response': responses\n","})\n","\n","# Remove duplicates\n","df = df.drop_duplicates()\n","\n","print('There are ' + str(len(df)) + ' successfully-generated examples.')\n","\n","# Initialize list to store training examples\n","training_examples = []\n","\n","# Create training examples in the format required for GPT-3.5 fine-tuning\n","for index, row in df.iterrows():\n","    training_example = {\n","        \"messages\": [\n","            {\"role\": \"system\", \"content\": system_message.strip()},\n","            {\"role\": \"user\", \"content\": row['prompt']},\n","            {\"role\": \"assistant\", \"content\": row['response']}\n","        ]\n","    }\n","    training_examples.append(training_example)\n","\n","# Save training examples to a .jsonl file\n","with open('/content/drive/MyDrive/Colab Notebooks/Gpt Fine Tuning/training_examples 7.jsonl', 'w') as f:\n","    for example in training_examples:\n","        f.write(json.dumps(example) + '\\n')"],"metadata":{"id":"7CEdkYeRsdmB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700502039266,"user_tz":-300,"elapsed":1155,"user":{"displayName":"Muhammad Bilal Haneef Qureshi","userId":"09555493137991746485"}},"outputId":"032931c0-5d66-4aa1-b3e7-2c6ba84fd1d1"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 5 successfully-generated examples.\n"]}]},{"cell_type":"markdown","source":["# Upload the file to OpenAI"],"metadata":{"id":"KWTY6qVgXD_T"}},{"cell_type":"code","source":["file_id = openai.File.create(\n","  file=open(\"/content/drive/MyDrive/Colab Notebooks/Gpt Fine Tuning/output.jsonl\", \"rb\"),\n","  purpose='fine-tune'\n",").id"],"metadata":{"id":"4LjEUrI9XDgT","executionInfo":{"status":"ok","timestamp":1700561727668,"user_tz":-300,"elapsed":1294,"user":{"displayName":"Muhammad Bilal Haneef Qureshi","userId":"09555493137991746485"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["# Train the model! You may need to wait a few minutes before running the next cell to allow for the file to process on OpenAI's servers."],"metadata":{"id":"HmYRIq8dW9IR"}},{"cell_type":"code","source":["job = openai.FineTuningJob.create(training_file=file_id, model=\"gpt-3.5-turbo\")\n","\n","job_id = job.id"],"metadata":{"id":"rdEyXmkoW80I","colab":{"base_uri":"https://localhost:8080/","height":374},"executionInfo":{"status":"error","timestamp":1700505372501,"user_tz":-300,"elapsed":717,"user":{"displayName":"Muhammad Bilal Haneef Qureshi","userId":"09555493137991746485"}},"outputId":"fb816996-edd8-44ba-90e2-8e1383b1f307"},"execution_count":10,"outputs":[{"output_type":"error","ename":"InvalidRequestError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-9945df97ca5c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFineTuningJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mjob_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/createable_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m     55\u001b[0m         )\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             )\n","\u001b[0;31mInvalidRequestError\u001b[0m: Fine-tuning jobs cannot be created on an Explore plan. You can upgrade to a paid plan on your billing page: https://platform.openai.com/account/billing/overview"]}]},{"cell_type":"markdown","source":["# Now, just wait until the fine-tuning run is done, and you'll have a ready-to-use model!\n","\n","Run this cell every 20 minutes or so -- eventually, you'll see a message \"New fine-tuned model created: ft:gpt-3.5-turbo-0613:xxxxxxxxxxxx\"\n","\n","Once you see that message, you can go to the OpenAI Playground (or keep going to the next cells and use the API) to try the model!"],"metadata":{"id":"XUSX5QzmZMTd"}},{"cell_type":"code","source":["openai.FineTuningJob.list_events(id=job_id, limit=10)"],"metadata":{"id":"45DJZ7hHaBx0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Once your model is trained, run the next cell to grab the fine-tuned model name."],"metadata":{"id":"91ihW2O27Phl"}},{"cell_type":"code","source":["model_name_pre_object = openai.FineTuningJob.retrieve(job_id)\n","model_name = model_name_pre_object.fine_tuned_model\n","print(model_name)"],"metadata":{"id":"eWBRBPh8aEzH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Let's try it out!"],"metadata":{"id":"2OmZLoBX7oQM"}},{"cell_type":"code","source":["response = openai.ChatCompletion.create(\n","    model=model_name,\n","    messages=[\n","      {\n","        \"role\": \"system\",\n","        \"content\": system_message,\n","      },\n","      {\n","          \"role\": \"user\",\n","          \"content\": df['prompt'].sample().values[0],\n","      }\n","    ],\n",")\n","\n","response.choices[0].message['content']"],"metadata":{"id":"uxbrmzc5dMuC"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"16e_HJwCHNsATFVtljZTMg0JydrHAhO8A","timestamp":1700480152165},{"file_id":"1NLqxHHCv3kFyw45t8k_CUfNlcepMdeDW","timestamp":1693271629443},{"file_id":"1h2lA5m9uCaPv91-Q8FWoNOBMOuUqyPoG","timestamp":1692813793327},{"file_id":"1mV9sAY4QBKLmS58dpFGHgwCXQKRASR31","timestamp":1692742949792},{"file_id":"1LOwhFY5rFzA2f3O2ZaY8vn3oh_nuqzGy","timestamp":1691595101338},{"file_id":"1Zmaceu65d7w4Tcd-cfnZRb6k_Tcv2b8g","timestamp":1691590111735},{"file_id":"1CSSeSBs4ki99r2LI5d3cT6qvszfB2VfQ","timestamp":1691513246004},{"file_id":"1iRqQyDAbnS0qYTajA_uVmv96lBz5N65n","timestamp":1691511454214},{"file_id":"1Y1YzhGDDeE1D1BOYCXAEsIU_WqTNVVSB","timestamp":1691507164920}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}